{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b57f132",
   "metadata": {},
   "source": [
    "# Элементы кода для получения датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "#path_for_token_mfd = Path(pathlib.Path.cwd().parent, 'parsing_folder', 'mfd_sber_downloaded.csv')\n",
    "names=['author', 'text_initial', 'text_date', 'url']\n",
    "\n",
    "from natasha import (Segmenter, NewsEmbedding, NewsMorphTagger, NewsSyntaxParser, Doc, MorphVocab)    \n",
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "morph_vocab = MorphVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_for_token_mfd = Path(pathlib.Path.cwd().parent, 'parsing_folder', 'mfd_sber_downloaded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/sammy/servers/myproject/parsing_folder/mfd_sber_downloaded.csv')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_for_token_mfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_text_by_natasha(text_initial):\n",
    "    try:\n",
    "        tokenized_text = []\n",
    "        doc = Doc(text_initial)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "        doc.parse_syntax(syntax_parser)\n",
    "        for token1 in doc.tokens:\n",
    "            token1.lemmatize(morph_vocab)\n",
    "            tokenized_text.append(token1.lemma)\n",
    "        tokenized_text = [x for x in tokenized_text if x and x not in '- \\t\\n.,;:!?(—)«»<>„“…+..?…?/©']\n",
    "        return tokenized_text\n",
    "    except:\n",
    "        return text_initial\n",
    "\n",
    "def df_with_tokens_and_date(df_from_csv):\n",
    "    df_temp = df_from_csv.copy()\n",
    "    df_temp['tokenized_text'] = df_temp.text_initial.apply(get_tokenized_text_by_natasha)\n",
    "    from collections import defaultdict\n",
    "    dates = defaultdict(list)\n",
    "    dates2 = defaultdict(str)\n",
    "    for i in range(len(df_temp)):\n",
    "        dates[df_temp.text_date.iloc[i]].extend(df_temp.tokenized_text.iloc[i])\n",
    "        dates2[df_temp.text_date.iloc[i]] = dates2[df_temp.text_date.iloc[i]] + df_temp.text_initial.iloc[i]\n",
    "    df_with_tokens = pd.DataFrame(index=pd.DatetimeIndex(dates.keys()), data=None)\n",
    "    df_with_tokens['text'] = dates.values()\n",
    "    df_with_tokens['text_initial'] = dates2.values()\n",
    "    df_with_tokens.index.name = 'date'\n",
    "    df_with_tokens.sort_index(inplace=True)\n",
    "    return df_with_tokens\n",
    "\n",
    "def df_from_csv_and_dropna(path_for_token_mfd , names):\n",
    "    df = pd.read_csv(path_for_token_mfd, names=names)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "def rename_date(date):\n",
    "    return str(date)[-4:] + '-' + str(date)[-7:-5] + '-' + str(date)[-10:-8]\n",
    "\n",
    "def get_mfd_with_tokens(path_for_token_mfd):\n",
    "    df_from_csv = df_from_csv_and_dropna(path_for_token_mfd,names)\n",
    "    df_from_csv.reset_index(drop=True, inplace=True)\n",
    "    df_from_csv['text_date'] = df_from_csv['text_date'].apply(rename_date)\n",
    "    df_with_tokens = df_with_tokens_and_date(df_from_csv)\n",
    "    df_with_tokens.to_pickle('mfd_with_tokens.pkl')\n",
    "    df_with_tokens.to_csv('mfd_with_tokens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_csv = df_from_csv_and_dropna(path_for_token_mfd,names)\n",
    "df_from_csv.reset_index(drop=True, inplace=True)\n",
    "df_from_csv['text_date'] = df_from_csv['text_date'].apply(rename_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_tokens = df_with_tokens_and_date(df_from_csv)\n",
    "#df_with_tokens.to_pickle('mfd_with_tokens.pkl')\n",
    "#df_with_tokens.to_csv('mfd_with_tokens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = './project/smartlab_comments_sber.csv'\n",
    "# names=['author', 'text_initial', 'text_date', 'url']\n",
    "\n",
    "# ДЛЯ МФД ЗАПУСКАЕМ:\n",
    "path = './project/mfd_sber.csv'\n",
    "path2 = './project/mfd_sber_part2.csv'\n",
    "names=['author', 'text_initial', 'text_date', 'url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_csv_and_dropna(path, names):\n",
    "    df = pd.read_csv(path, names=names)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "df_temp_csv = df_from_csv_and_dropna(path,names)\n",
    "df_temp_csv2 = df_from_csv_and_dropna(path2,names)\n",
    "#df_from_csv = df_from_csv[-5000:]\n",
    "\n",
    "df_from_csv = pd.concat([df_temp_csv, df_temp_csv2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_csv.reset_index(drop=True, inplace=True)\n",
    "#df_from_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_date(date):\n",
    "    return str(date)[-4:] + '-' + str(date)[-7:-5] + '-' + str(date)[-10:-8]\n",
    "\n",
    "# ЗАПУСКАЕМ ТОЛЬКО ДЛЯ MFD\n",
    "df_from_csv['text_date'] = df_from_csv['text_date'].apply(rename_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (Segmenter, NewsEmbedding, NewsMorphTagger, NewsSyntaxParser, Doc, MorphVocab)    \n",
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "morph_vocab = MorphVocab()\n",
    "def get_tokenized_text_by_natasha(text_initial):\n",
    "    try:\n",
    "        tokenized_text = []\n",
    "        doc = Doc(text_initial)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "        doc.parse_syntax(syntax_parser)\n",
    "        for token1 in doc.tokens:\n",
    "            token1.lemmatize(morph_vocab)\n",
    "            tokenized_text.append(token1.lemma)\n",
    "        tokenized_text = [x for x in tokenized_text if x and x not in '- \\t\\n.,;:!?(—)«»<>„“…+..?…?/©']\n",
    "        return tokenized_text\n",
    "    except:\n",
    "        return text_initial\n",
    "\n",
    "def df_with_tokens_and_date(df_from_csv):\n",
    "    df_temp = df_from_csv.copy()\n",
    "    df_temp['tokenized_text'] = df_temp.text_initial.apply(get_tokenized_text_by_natasha)\n",
    "    from collections import defaultdict\n",
    "    dates = defaultdict(list)\n",
    "    dates2 = defaultdict(str)\n",
    "    for i in range(len(df_temp)):\n",
    "        dates[df_temp.text_date.iloc[i]].extend(df_temp.tokenized_text.iloc[i])\n",
    "        dates2[df_temp.text_date.iloc[i]] = dates2[df_temp.text_date.iloc[i]] + df_temp.text_initial.iloc[i]\n",
    "    df_with_tokens = pd.DataFrame(index=pd.DatetimeIndex(dates.keys()), data=None)\n",
    "    df_with_tokens['text'] = dates.values()\n",
    "    df_with_tokens['text_initial'] = dates2.values()\n",
    "    df_with_tokens.index.name = 'date'\n",
    "    df_with_tokens.sort_index(inplace=True)\n",
    "    return df_with_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_tokens = df_with_tokens_and_date(df_from_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_with_tokens_and_price(df_with_tokens):\n",
    "    df_temp = df_with_tokens.copy()\n",
    "    target = pd.read_csv('sbertarget.csv', index_col='Date')\n",
    "    datetime_index = pd.DatetimeIndex(target.index)\n",
    "    target.index = datetime_index\n",
    "    df_temp['price'] = target['Price']\n",
    "    df_temp.price.fillna(0, inplace=True)\n",
    "    for i in range(len(df_temp)-1):\n",
    "        if df_temp.price.iloc[i] == 0 and df_temp.price.iloc[i+1] == 0:\n",
    "            df_temp.text.iloc[i-1].extend(df_temp.text.iloc[i])   #.extend(df2.text.iloc[i+1]))\n",
    "            df_temp.text.iloc[i-1].extend(df_temp.text.iloc[i+1])\n",
    "    df_with_tokens_and_price = df_temp[df_temp.price!=0]\n",
    "    \n",
    "    return df_with_tokens_and_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_tokens_and_price = get_df_with_tokens_and_price(df_with_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(df_with_tokens_and_price):\n",
    "    df_temp = df_with_tokens_and_price.copy()\n",
    "    target_real = pd.read_csv('df_price.csv', names=['date', 'price', 'class'])\n",
    "    datetime_series = pd.to_datetime(target_real['date'])\n",
    "    datetime_index = pd.DatetimeIndex(datetime_series.values)\n",
    "    target_real.index = datetime_index\n",
    "    df_temp['target_real'] = target_real['class']\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_dataset_smartlab = get_dataset(df_with_tokens_and_price)\n",
    "df_dataset_mfd = get_dataset(df_with_tokens_and_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_dataset_mfd.to_pickle('df_dataset_mfd.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
